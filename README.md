# Word2Vec Model on Wikipedia Articles.

### Download Training Data from https://dumps.wikimedia.org/enwiki/latest/ 

#### I have trained the Model on using enwiki-latest-pages-articles.xml.bz2, but there are a lot of other dataset available. Total size of this dataset after extraction is 11GB. 
